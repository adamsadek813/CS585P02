{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "\n",
    "TRAIN_SIZE = 80.0\n",
    "\n",
    "# get user input string\n",
    "# user_input = input(\"Enter a review: \")\n",
    "\n",
    "# initialize TRAIN_SIZE (from user)\n",
    "input_train_size = int(input(\"Enter the percentage of data to train on (20-80): \"))\n",
    "\n",
    "#input_train_size = sys.argv[1]\n",
    "if input_train_size < 20 or input_train_size > 80:\n",
    "    TRAIN_SIZE = 80\n",
    "else:\n",
    "   TRAIN_SIZE = input_train_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_pre = pd.read_csv(\"bumble_google_play_reviews.csv\")\n",
    "data = data_pre.dropna(subset=['content']).reset_index(drop=True)\n",
    "data['review_class'] = data['score'].apply(lambda x: 'positive' if x > 2 else 'negative')\n",
    "\n",
    "\n",
    "# 80-20 split\n",
    "test_index = int(len(data) * 0.8)\n",
    "train_index = int(len(data) * float(TRAIN_SIZE / 100.0))\n",
    "train_data = data.iloc[:train_index]\n",
    "test_data = data.iloc[test_index:]\n",
    "\n",
    "pos_indices = []\n",
    "neg_indices = []\n",
    "\n",
    "for x in range(len(train_data)):\n",
    "    if train_data['review_class'][x] == 'positive':\n",
    "        pos_indices.append(x)\n",
    "    if train_data['review_class'][x] == 'negative':\n",
    "        neg_indices.append(x)\n",
    "\n",
    "\n",
    "prior_probabilities = {\n",
    "    'positive': len(train_data[train_data['review_class'] == 'positive']) / len(train_data),\n",
    "    'negative': len(train_data[train_data['review_class'] == 'negative']) / len(train_data)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "# list of common stop words\n",
    "stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with']\n",
    "\n",
    "\n",
    "translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "for x in range(len(data['content'])):\n",
    "    content = data['content'][x]\n",
    "    lowercase_content = content.lower()\n",
    "    content_no_punct = lowercase_content.translate(translation_table)\n",
    "    vocabulary.extend(content_no_punct.split())\n",
    "\n",
    "\n",
    "\n",
    "unique_vocabulary = []\n",
    "for item in vocabulary:\n",
    "    if item not in unique_vocabulary and item not in stop_words:\n",
    "        unique_vocabulary.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 123, 3, 37, 1, 108, 8, 14, 4, 56, 21, 32, 20, 29, 35, 17, 38, 44, 18, 10, 6, 54, 32, 1, 10, 9, 30, 16, 25, 12, 20, 15, 37, 24, 3, 1, 5, 4, 5, 21, 20, 14, 23, 3, 12, 36, 4, 2, 8, 3, 1, 1, 1, 7, 12, 1, 6, 1, 71, 22, 6, 5, 14, 2, 4, 8, 22, 4, 1, 6, 1, 7, 8, 2, 23, 7, 13, 9, 12, 9, 17, 4, 3, 3, 8, 6, 1, 13, 27, 10, 2, 3, 5, 1, 1, 11, 2, 2, 25, 1, 2, 1, 7, 18, 1, 6, 25, 14, 3, 9, 10, 1, 1, 2, 3, 5, 2, 1, 1, 3, 17, 9, 11, 1, 11, 7, 7, 3, 3, 1, 1, 1, 12, 7, 15, 3, 4, 7, 22, 1, 13, 5, 9, 1, 6, 8, 1, 42, 7, 2, 21, 42, 40, 7, 3, 7, 14, 5, 4, 1, 3, 14, 3, 14, 23, 3, 14, 3, 7, 12, 3, 1, 6, 12, 6, 11, 16, 1, 1, 4, 3, 3, 2, 1, 23, 9, 5, 4, 2, 13, 9, 14, 11, 2, 13, 1, 4, 11, 2, 24, 10, 14, 10, 13, 1, 24, 6, 1, 13, 2, 1, 1, 9, 7, 1, 3, 1, 7, 3, 1, 4, 2, 2, 1, 1, 20, 2, 5, 1, 8, 1, 1, 2, 4, 1, 4, 13, 2, 3, 1, 4, 1, 4, 19, 7, 2, 2, 2, 4, 2, 1, 38, 5, 3, 5, 2, 2, 4, 1, 4, 1, 8, 3, 2, 6, 6, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 15, 2, 3, 1, 20, 7, 6, 1, 3, 8, 14, 3, 4, 17, 2, 2, 5, 1, 20, 1, 2, 3, 1, 25, 1, 1, 1, 2, 1, 1, 1, 10, 7, 6, 3, 10, 4, 5, 7, 13, 5, 2, 6, 1, 3, 1, 1, 3, 2, 13, 2, 1, 7, 1, 1, 2, 6, 21, 12, 1, 24, 2, 1, 1, 5, 2, 1, 2, 1, 1, 5, 2, 3, 2, 1, 3, 4, 9, 7, 4, 1, 1, 2, 1, 1, 1, 1, 6, 5, 1, 2, 2, 5, 1, 1, 1, 1, 2, 3, 3, 1, 8, 5, 2, 1, 1, 1, 3, 9, 1, 2, 3, 1, 2, 1, 5, 5, 1, 1, 3, 10, 10, 6, 2, 6, 1, 2, 6, 2, 5, 2, 3, 8, 5, 3, 1, 1, 1, 5, 1, 8, 8, 1, 5, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 10, 3, 1, 3, 5, 4, 1, 1, 1, 4, 1, 1, 1, 2, 1, 4, 1, 1, 7, 1, 1, 1, 1, 15, 1, 1, 4, 1, 1, 1, 2, 5, 1, 7, 18, 1, 2, 1, 1, 1, 1, 1, 2, 5, 3, 9, 6, 1, 1, 1, 1, 1, 1, 5, 4, 3, 4, 2, 3, 1, 3, 5, 4, 1, 7, 1, 1, 1, 1, 1, 6, 4, 1, 4, 6, 2, 3, 1, 2, 5, 7, 2, 4, 1, 1, 7, 1, 12, 1, 1, 4, 1, 1, 7, 1, 8, 2, 3, 1, 2, 1, 1, 4, 2, 12, 2, 1, 1, 1, 1, 2, 3, 9, 1, 1, 1, 2, 9, 1, 1, 2, 1, 2, 1, 2, 1, 1, 4, 2, 4, 2, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 8, 1, 4, 9, 4, 3, 7, 3, 2, 3, 1, 2, 1, 3, 1, 2, 2, 3, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 3, 5, 3, 5, 3, 6, 2, 3, 1, 1, 1, 1, 1, 4, 2, 2, 5, 5, 6, 1, 9, 1, 1, 7, 7, 5, 4, 9, 4, 1, 1, 1, 1, 1, 17, 1, 1, 1, 1, 1, 1, 3, 1, 3, 2, 1, 4, 4, 1, 2, 19, 5, 1, 1, 3, 1, 1, 1, 1, 1, 16, 2, 1, 1, 1, 3, 2, 2, 4, 1, 2, 1, 3, 1, 2, 4, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 6, 1, 2, 1, 1, 1, 1, 1, 1, 1, 5, 1, 3, 4, 1, 3, 6, 1, 1, 3, 2, 2, 7, 6, 2, 2, 6, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 4, 1, 2, 1, 8, 4, 3, 2, 4, 1, 1, 6, 4, 1, 1, 1, 1, 5, 1, 3, 1, 5, 2, 1, 1, 1, 2, 2, 1, 3, 2, 3, 2, 1, 5, 5, 1, 2, 2, 1, 5, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 2, 5, 1, 1, 3, 3, 1, 1, 6, 3, 1, 1, 1, 9, 1, 1, 3, 4, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 1, 3, 14, 1, 1, 1, 1, 2, 1, 1, 1, 1, 7, 1, 1, 3, 2, 1, 2, 1, 1, 3, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 7, 1, 2, 4, 2, 1, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 5, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 4, 3, 3, 4, 2, 2, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 3, 1, 1, 3, 1, 1, 1, 2, 2, 1, 2, 2, 3, 4, 1, 1, 2, 4, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 2, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2, 1, 1, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 5, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[19, 33, 2, 6, 0, 21, 1, 3, 0, 13, 1, 1, 4, 3, 8, 1, 8, 5, 4, 1, 0, 11, 2, 0, 3, 1, 5, 3, 5, 1, 1, 3, 10, 6, 0, 0, 1, 0, 0, 5, 8, 5, 3, 1, 1, 2, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 12, 6, 3, 2, 2, 1, 1, 2, 3, 1, 1, 1, 1, 2, 5, 1, 5, 1, 5, 4, 4, 1, 2, 2, 2, 2, 2, 1, 1, 10, 8, 0, 0, 0, 2, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 3, 1, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 5, 4, 2, 0, 0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 0, 6, 1, 0, 3, 6, 4, 0, 0, 2, 5, 2, 0, 0, 1, 2, 0, 0, 6, 0, 3, 0, 0, 2, 0, 0, 0, 1, 3, 1, 13, 1, 1, 2, 2, 1, 1, 1, 2, 3, 3, 1, 1, 0, 1, 0, 4, 1, 2, 1, 1, 4, 1, 4, 2, 3, 5, 3, 1, 7, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 5, 1, 0, 1, 0, 1, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 6, 1, 1, 0, 0, 0, 2, 0, 0, 5, 0, 1, 1, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 7, 0, 0, 1, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 3, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 4, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[8, 90, 1, 31, 1, 87, 7, 11, 4, 43, 20, 31, 16, 26, 27, 16, 30, 39, 14, 9, 6, 43, 30, 1, 7, 8, 25, 13, 20, 11, 19, 12, 27, 18, 3, 1, 4, 4, 5, 16, 12, 9, 20, 2, 11, 34, 4, 1, 6, 2, 0, 0, 0, 6, 11, 0, 4, 0, 59, 16, 3, 3, 12, 1, 3, 6, 19, 3, 0, 5, 0, 5, 3, 1, 18, 6, 8, 5, 8, 8, 15, 2, 1, 1, 6, 5, 0, 3, 19, 10, 2, 3, 3, 1, 1, 11, 2, 2, 18, 1, 2, 1, 6, 16, 1, 6, 24, 14, 2, 6, 9, 1, 1, 1, 2, 3, 2, 1, 1, 2, 16, 9, 10, 1, 10, 7, 7, 2, 3, 1, 1, 1, 7, 3, 13, 3, 4, 5, 20, 1, 11, 5, 8, 1, 6, 7, 1, 36, 6, 2, 18, 36, 36, 7, 3, 5, 9, 3, 4, 1, 2, 12, 3, 14, 17, 3, 11, 3, 7, 10, 3, 1, 6, 11, 3, 10, 3, 0, 0, 2, 1, 2, 1, 0, 21, 6, 2, 3, 1, 13, 8, 14, 7, 1, 11, 0, 3, 7, 1, 20, 8, 11, 5, 10, 0, 17, 4, 0, 11, 1, 0, 0, 8, 5, 0, 2, 0, 5, 1, 0, 3, 1, 1, 0, 0, 16, 1, 4, 0, 7, 0, 1, 2, 4, 1, 3, 12, 2, 3, 1, 4, 1, 3, 14, 6, 2, 1, 2, 3, 2, 1, 35, 5, 2, 5, 2, 2, 4, 1, 4, 1, 8, 3, 2, 6, 6, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 10, 2, 3, 1, 14, 6, 5, 1, 3, 8, 12, 3, 4, 12, 2, 1, 4, 1, 17, 1, 2, 3, 1, 24, 1, 1, 1, 2, 1, 1, 1, 8, 5, 4, 3, 8, 3, 4, 7, 11, 5, 2, 6, 1, 3, 1, 1, 3, 2, 12, 2, 1, 7, 1, 1, 2, 6, 20, 6, 1, 22, 2, 1, 1, 4, 2, 1, 2, 1, 1, 5, 2, 2, 2, 1, 3, 4, 7, 6, 3, 0, 0, 2, 1, 1, 1, 1, 6, 3, 1, 1, 2, 4, 1, 1, 1, 1, 1, 2, 2, 0, 5, 4, 1, 0, 0, 0, 2, 8, 0, 2, 2, 1, 2, 1, 4, 4, 1, 1, 3, 7, 9, 6, 2, 5, 1, 2, 6, 2, 5, 2, 2, 8, 4, 2, 1, 1, 1, 5, 1, 7, 7, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 9, 3, 1, 3, 5, 4, 1, 1, 1, 3, 1, 1, 1, 2, 1, 4, 1, 1, 6, 1, 1, 1, 1, 12, 1, 1, 4, 1, 1, 1, 2, 5, 1, 5, 17, 1, 2, 1, 1, 1, 1, 1, 2, 4, 3, 8, 6, 1, 1, 1, 1, 1, 1, 5, 4, 3, 2, 2, 3, 1, 3, 4, 2, 1, 4, 1, 1, 1, 1, 1, 5, 3, 1, 1, 6, 2, 3, 1, 2, 4, 7, 2, 3, 0, 0, 6, 0, 9, 0, 0, 2, 0, 1, 6, 1, 7, 1, 2, 1, 2, 1, 1, 2, 2, 11, 2, 1, 1, 1, 1, 2, 3, 5, 1, 1, 1, 2, 6, 0, 1, 2, 1, 1, 1, 1, 1, 1, 4, 2, 4, 1, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 7, 1, 4, 7, 4, 3, 7, 3, 2, 3, 1, 0, 0, 2, 0, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 4, 2, 4, 2, 5, 1, 2, 0, 0, 0, 0, 0, 3, 0, 1, 5, 5, 6, 1, 2, 1, 1, 6, 6, 5, 3, 8, 2, 1, 1, 1, 1, 1, 13, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 1, 4, 4, 1, 2, 18, 5, 1, 1, 2, 1, 1, 1, 1, 1, 13, 2, 1, 1, 1, 3, 1, 2, 4, 1, 2, 1, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 5, 1, 2, 1, 1, 1, 0, 0, 0, 1, 5, 1, 3, 4, 1, 3, 6, 1, 1, 3, 2, 2, 7, 4, 2, 2, 5, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 4, 1, 2, 1, 8, 3, 3, 2, 4, 0, 0, 5, 3, 1, 1, 1, 1, 4, 1, 3, 1, 4, 2, 1, 1, 1, 1, 2, 1, 3, 2, 3, 2, 1, 4, 5, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 2, 5, 1, 1, 3, 3, 1, 1, 5, 3, 1, 1, 1, 8, 1, 1, 3, 3, 1, 1, 1, 2, 3, 1, 1, 0, 2, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 2, 10, 0, 1, 1, 1, 1, 1, 0, 0, 0, 6, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 0, 3, 1, 2, 3, 2, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 0, 0, 1, 3, 1, 3, 1, 3, 1, 1, 1, 0, 0, 0, 4, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 5, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 4, 3, 3, 4, 2, 2, 0, 0, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 3, 4, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 3, 1, 0, 0, 1, 1, 0, 0, 2, 2, 3, 1, 0, 1, 3, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, 2, 2, 2, 0, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "binary_bag = [[0]*len(unique_vocabulary) for _ in range(len(train_data))]\n",
    "\n",
    "for x in range(len(train_data)):\n",
    "    for y in range(len(train_data['content'][x].split())):\n",
    "        words_in_one_sentence = (train_data['content'][x].split()[y]).lower().translate(translation_table)\n",
    "        if words_in_one_sentence != '' and words_in_one_sentence not in stop_words:   \n",
    "            index_of_word = unique_vocabulary.index(words_in_one_sentence)\n",
    "            binary_bag[x][index_of_word] = 1\n",
    "\n",
    "# sigma(z) = 1 / (1 + e^(-z))\n",
    "\n",
    "num_docs_including_word = [0]*len(unique_vocabulary)\n",
    "num_pos_docs_including_word = [0]*len(unique_vocabulary)\n",
    "num_neg_docs_including_word = [0]*len(unique_vocabulary)\n",
    "\n",
    "for word in range(len(unique_vocabulary)):\n",
    "    for document in range(len(binary_bag)):\n",
    "        if binary_bag[document][word] == 1:\n",
    "            num_docs_including_word[word] += 1\n",
    "            if document in pos_indices:\n",
    "                num_pos_docs_including_word[word] += 1\n",
    "            if document in neg_indices:\n",
    "                num_neg_docs_including_word[word] += 1\n",
    "\n",
    "\n",
    "print(num_docs_including_word)\n",
    "print(num_pos_docs_including_word)\n",
    "print(num_neg_docs_including_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(binary_bag[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_word_given_pos_class = []\n",
    "prob_word_given_neg_class = []\n",
    "\n",
    "count_pos_class = math.fsum(num_pos_docs_including_word)\n",
    "count_neg_class = math.fsum(num_neg_docs_including_word)\n",
    "\n",
    "\n",
    "for x in range(len(unique_vocabulary)):\n",
    "    prob_word_given_pos_class.append(float(num_pos_docs_including_word[x] + 1) / float(count_pos_class + len(unique_vocabulary)))\n",
    "    prob_word_given_neg_class.append(float(num_neg_docs_including_word[x] + 1) / float(count_neg_class + len(unique_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "def naive_bayes_classifier(test_data):\n",
    "    predictions = []\n",
    "    # number of unique words in the vocabulary\n",
    "    V = len(unique_vocabulary)\n",
    "    # smoothing factor\n",
    "    alpha = 1\n",
    "    \n",
    "    for x in range(len(test_data)):\n",
    "        pos_prob = math.log(prior_probabilities['positive'])\n",
    "        neg_prob = math.log(prior_probabilities['negative'])\n",
    "        document_words = test_data['content'][x].split()\n",
    "\n",
    "        for word in document_words:\n",
    "            word = word.lower().translate(translation_table)\n",
    "            if word != '' and word not in stop_words:\n",
    "                if word in unique_vocabulary:\n",
    "                    index_of_word = unique_vocabulary.index(word)\n",
    "                    # Calculate word probability with smoothing\n",
    "                    word_pos_prob = (num_pos_docs_including_word[index_of_word] + alpha) / (len(pos_indices) + alpha * V)\n",
    "                    word_neg_prob = (num_neg_docs_including_word[index_of_word] + alpha) / (len(neg_indices) + alpha * V)\n",
    "                else:\n",
    "                    # Apply smoothing for words not in the vocabulary\n",
    "                    word_pos_prob = alpha / (len(pos_indices) + alpha * V)\n",
    "                    word_neg_prob = alpha / (len(neg_indices) + alpha * V)\n",
    "                \n",
    "                pos_prob += math.log(word_pos_prob)\n",
    "                neg_prob += math.log(word_neg_prob)\n",
    "                \n",
    "                # convert to linear space so that we can compare the probabilities (add up to 1)\n",
    "                pos_prob = 10**pos_prob\n",
    "                neg_prob = 10**neg_prob\n",
    "                \n",
    "        if pos_prob >= neg_prob:\n",
    "            predictions.append('positive')\n",
    "        else:\n",
    "            predictions.append('negative')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor for a particular review\n",
    "def naive_bayes_individual(review):\n",
    "    # number of unique words in the vocabulary\n",
    "    V = len(unique_vocabulary)\n",
    "    # alpha is the smoothing factor\n",
    "    alpha = 1\n",
    "    \n",
    "    pos_prob = math.log(prior_probabilities['positive'])\n",
    "    neg_prob = math.log(prior_probabilities['negative'])\n",
    "    document_words = review.split()\n",
    "\n",
    "    for word in document_words:\n",
    "        word = word.lower().translate(translation_table)\n",
    "        if word != '' and word not in stop_words: \n",
    "            if word in unique_vocabulary:\n",
    "                index_of_word = unique_vocabulary.index(word)\n",
    "                word_pos_prob = (num_pos_docs_including_word[index_of_word] + alpha) / (len(pos_indices) + V)\n",
    "                word_neg_prob = (num_neg_docs_including_word[index_of_word] + alpha) / (len(neg_indices) + V)\n",
    "            else:\n",
    "                word_pos_prob = 1 / (len(pos_indices) + V)\n",
    "                word_neg_prob = 1 / (len(neg_indices) + V)\n",
    "            \n",
    "            pos_prob += math.log(word_pos_prob)\n",
    "            neg_prob += math.log(word_neg_prob)\n",
    "            \n",
    "            # convert to linear space so that we can compare the probabilities (add up to 1)\n",
    "            pos_prob = 10**pos_prob\n",
    "            neg_prob = 10**neg_prob\n",
    "\n",
    "    if pos_prob >= neg_prob:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract actual labels\n",
    "actual_labels = test_data['review_class'].tolist()\n",
    "# get predictions\n",
    "predictions = naive_bayes_classifier(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 17\n",
      "TN: 37\n",
      "FP: 12\n",
      "FN: 14\n",
      "Sensitivity (Recall): 0.5483870967741935\n",
      "Specificity: 0.7551020408163265\n",
      "Precision: 0.5862068965517241\n",
      "Negative Predictive Value: 0.7254901960784313\n",
      "Accuracy: 0.675\n",
      "F-score: 0.5666666666666665\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(predictions, actual_labels):\n",
    "    # Initialize counters\n",
    "    TP = FP = TN = FN = 0\n",
    "    \n",
    "    # Count occurrences\n",
    "    for pred, actual in zip(predictions, actual_labels):\n",
    "        if pred == 'positive' and actual == 'positive':\n",
    "            TP += 1\n",
    "        elif pred == 'positive' and actual == 'negative':\n",
    "            FP += 1\n",
    "        elif pred == 'negative' and actual == 'positive':\n",
    "            FN += 1\n",
    "        elif pred == 'negative' and actual == 'negative':\n",
    "            TN += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sensitivity = TP / (TP + FN) if TP + FN else 0  \n",
    "    specificity = TN / (TN + FP) if TN + FP else 0\n",
    "    precision = TP / (TP + FP) if TP + FP else 0\n",
    "    negative_predictive_value = TN / (TN + FN) if TN + FN else 0\n",
    "    accuracy = (TP + TN) / (TP + FP + FN + TN) if TP + FP + FN + TN else 0\n",
    "    F_score = (2 * precision * sensitivity) / (precision + sensitivity) if precision + sensitivity else 0\n",
    "\n",
    "    return {\n",
    "        'TP': TP,\n",
    "        'TN': TN,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'Sensitivity (Recall)': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Negative Predictive Value': negative_predictive_value,\n",
    "        'Accuracy': accuracy,\n",
    "        'F-score': F_score\n",
    "    }\n",
    "\n",
    "metrics = calculate_metrics(predictions, actual_labels)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import string\n",
    "import math\n",
    "\n",
    "def naive_bayes_train(train_data):\n",
    "    # get user input string\n",
    "    # user_input = input(\"Enter a review: \")\n",
    "\n",
    "    # initialize TRAIN_SIZE (from user)\n",
    "    input_train_size = int(input(\"Enter the percentage of data to train on (20-80): \"))\n",
    "\n",
    "    #input_train_size = sys.argv[1]\n",
    "    if input_train_size < 20 or input_train_size > 80:\n",
    "        TRAIN_SIZE = 80\n",
    "    else:\n",
    "        TRAIN_SIZE = input_train_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data_pre = pd.read_csv(\"bumble_google_play_reviews.csv\")\n",
    "    data = data_pre.dropna(subset=['content']).reset_index(drop=True)\n",
    "    data['review_class'] = data['score'].apply(lambda x: 'positive' if x > 2 else 'negative')\n",
    "\n",
    "\n",
    "    # 80-20 split\n",
    "    test_index = int(len(data) * 0.8)\n",
    "    train_index = int(len(data) * float(TRAIN_SIZE / 100.0))\n",
    "    train_data = data.iloc[:train_index]\n",
    "    test_data = data.iloc[test_index:]\n",
    "\n",
    "    pos_indices = []\n",
    "    neg_indices = []\n",
    "\n",
    "    for x in range(len(train_data)):\n",
    "        if train_data['review_class'][x] == 'positive':\n",
    "            pos_indices.append(x)\n",
    "        if train_data['review_class'][x] == 'negative':\n",
    "            neg_indices.append(x)\n",
    "\n",
    "\n",
    "    prior_probabilities = {\n",
    "        'positive': len(train_data[train_data['review_class'] == 'positive']) / len(train_data),\n",
    "        'negative': len(train_data[train_data['review_class'] == 'negative']) / len(train_data)\n",
    "    }\n",
    "    \n",
    "    vocabulary = []\n",
    "    # list of common stop words\n",
    "    stop_words = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with']\n",
    "\n",
    "\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for x in range(len(data['content'])):\n",
    "        content = data['content'][x]\n",
    "        lowercase_content = content.lower()\n",
    "        content_no_punct = lowercase_content.translate(translation_table)\n",
    "        vocabulary.extend(content_no_punct.split())\n",
    "\n",
    "\n",
    "\n",
    "    unique_vocabulary = []\n",
    "    for item in vocabulary:\n",
    "        if item not in unique_vocabulary and item not in stop_words:\n",
    "            unique_vocabulary.append(item)\n",
    "\n",
    "    binary_bag = [[0]*len(unique_vocabulary) for _ in range(len(train_data))]\n",
    "\n",
    "    for x in range(len(train_data)):\n",
    "        for y in range(len(train_data['content'][x].split())):\n",
    "            words_in_one_sentence = (train_data['content'][x].split()[y]).lower().translate(translation_table)\n",
    "            if words_in_one_sentence != '' and words_in_one_sentence not in stop_words:   \n",
    "                index_of_word = unique_vocabulary.index(words_in_one_sentence)\n",
    "                binary_bag[x][index_of_word] = 1\n",
    "\n",
    "    # sigma(z) = 1 / (1 + e^(-z))\n",
    "\n",
    "    num_docs_including_word = [0]*len(unique_vocabulary)\n",
    "    num_pos_docs_including_word = [0]*len(unique_vocabulary)\n",
    "    num_neg_docs_including_word = [0]*len(unique_vocabulary)\n",
    "\n",
    "    for word in range(len(unique_vocabulary)):\n",
    "        for document in range(len(binary_bag)):\n",
    "            if binary_bag[document][word] == 1:\n",
    "                num_docs_including_word[word] += 1\n",
    "                if document in pos_indices:\n",
    "                    num_pos_docs_including_word[word] += 1\n",
    "                if document in neg_indices:\n",
    "                    num_neg_docs_including_word[word] += 1\n",
    "    \n",
    "    prob_word_given_pos_class = []\n",
    "    prob_word_given_neg_class = []\n",
    "\n",
    "    count_pos_class = math.fsum(num_pos_docs_including_word)\n",
    "    count_neg_class = math.fsum(num_neg_docs_including_word)\n",
    "\n",
    "\n",
    "    for x in range(len(unique_vocabulary)):\n",
    "        prob_word_given_pos_class.append(float(num_pos_docs_including_word[x] + 1) / float(count_pos_class + len(unique_vocabulary)))\n",
    "        prob_word_given_neg_class.append(float(num_neg_docs_including_word[x] + 1) / float(count_neg_class + len(unique_vocabulary)))\n",
    "    \n",
    "    return prob_word_given_pos_class, prob_word_given_neg_class\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "def naive_bayes_classifier_no_stop_words(test_data):\n",
    "    predictions = []\n",
    "    # number of unique words in the vocabulary\n",
    "    V = len(unique_vocabulary)\n",
    "    # smoothing factor\n",
    "    alpha = 1\n",
    "    \n",
    "    for x in range(len(test_data)):\n",
    "        pos_prob = math.log(prior_probabilities['positive'])\n",
    "        neg_prob = math.log(prior_probabilities['negative'])\n",
    "        document_words = test_data['content'][x].split()\n",
    "\n",
    "        for word in document_words:\n",
    "            word = word.lower().translate(translation_table)\n",
    "            if word != '' and word not in stop_words:\n",
    "                if word in unique_vocabulary:\n",
    "                    index_of_word = unique_vocabulary.index(word)\n",
    "                    # Calculate word probability with smoothing\n",
    "                    word_pos_prob = (num_pos_docs_including_word[index_of_word] + alpha) / (len(pos_indices) + alpha * V)\n",
    "                    word_neg_prob = (num_neg_docs_including_word[index_of_word] + alpha) / (len(neg_indices) + alpha * V)\n",
    "                else:\n",
    "                    # Apply smoothing for words not in the vocabulary\n",
    "                    word_pos_prob = alpha / (len(pos_indices) + alpha * V)\n",
    "                    word_neg_prob = alpha / (len(neg_indices) + alpha * V)\n",
    "                \n",
    "                pos_prob += math.log(word_pos_prob)\n",
    "                neg_prob += math.log(word_neg_prob)\n",
    "                \n",
    "                # convert to linear space so that we can compare the probabilities (add up to 1)\n",
    "                pos_prob = 10**pos_prob\n",
    "                neg_prob = 10**neg_prob\n",
    "                \n",
    "        if pos_prob >= neg_prob:\n",
    "            predictions.append('positive')\n",
    "        else:\n",
    "            predictions.append('negative')\n",
    "    return predictions\n",
    "\n",
    "# predictor for a particular review\n",
    "def naive_bayes_individual_no_stop_words(review):\n",
    "    # number of unique words in the vocabulary\n",
    "    V = len(unique_vocabulary)\n",
    "    # alpha is the smoothing factor\n",
    "    alpha = 1\n",
    "    \n",
    "    pos_prob = math.log(prior_probabilities['positive'])\n",
    "    neg_prob = math.log(prior_probabilities['negative'])\n",
    "    document_words = review.split()\n",
    "\n",
    "    for word in document_words:\n",
    "        word = word.lower().translate(translation_table)\n",
    "        if word != '' and word not in stop_words: \n",
    "            if word in unique_vocabulary:\n",
    "                index_of_word = unique_vocabulary.index(word)\n",
    "                word_pos_prob = (num_pos_docs_including_word[index_of_word] + alpha) / (len(pos_indices) + V)\n",
    "                word_neg_prob = (num_neg_docs_including_word[index_of_word] + alpha) / (len(neg_indices) + V)\n",
    "            else:\n",
    "                word_pos_prob = 1 / (len(pos_indices) + V)\n",
    "                word_neg_prob = 1 / (len(neg_indices) + V)\n",
    "            \n",
    "            pos_prob += math.log(word_pos_prob)\n",
    "            neg_prob += math.log(word_neg_prob)\n",
    "            \n",
    "            # convert to linear space so that we can compare the probabilities (add up to 1)\n",
    "            pos_prob = 10**pos_prob\n",
    "            neg_prob = 10**neg_prob\n",
    "\n",
    "    if pos_prob >= neg_prob:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 17\n",
      "TN: 37\n",
      "FP: 12\n",
      "FN: 14\n",
      "Sensitivity (Recall): 0.5483870967741935\n",
      "Specificity: 0.7551020408163265\n",
      "Precision: 0.5862068965517241\n",
      "Negative Predictive Value: 0.7254901960784313\n",
      "Accuracy: 0.675\n",
      "F-score: 0.5666666666666665\n"
     ]
    }
   ],
   "source": [
    "# extract actual labels\n",
    "actual_labels = test_data['review_class'].tolist()\n",
    "# get predictions\n",
    "predictions = naive_bayes_classifier_no_stop_words(test_data)\n",
    "\n",
    "# get metrics\n",
    "metrics = calculate_metrics(predictions, actual_labels)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_train(train_data):\n",
    "    global unique_vocabulary\n",
    "    global prior_probabilities\n",
    "    global translation_table\n",
    "    global num_pos_docs_including_word\n",
    "    global num_neg_docs_including_word\n",
    "    global pos_indices\n",
    "    global neg_indices\n",
    "    \n",
    "    for x in range(len(train_data)):\n",
    "        if train_data['review_class'][x] == 'positive':\n",
    "            pos_indices.append(x)\n",
    "        if train_data['review_class'][x] == 'negative':\n",
    "            neg_indices.append(x)\n",
    "    \n",
    "    prior_probabilities = {\n",
    "        'positive': len(train_data[train_data['review_class'] == 'positive']) / len(train_data),\n",
    "        'negative': len(train_data[train_data['review_class'] == 'negative']) / len(train_data)\n",
    "    }\n",
    "    \n",
    "    vocabulary = []\n",
    "\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for x in range(len(data['content'])):\n",
    "        content = data['content'][x]\n",
    "        lowercase_content = content.lower()\n",
    "        content_no_punct = lowercase_content.translate(translation_table)\n",
    "        vocabulary.extend(content_no_punct.split())\n",
    "\n",
    "\n",
    "\n",
    "    for item in vocabulary:\n",
    "        if item not in unique_vocabulary:\n",
    "            unique_vocabulary.append(item)\n",
    "\n",
    "    binary_bag = [[0]*len(unique_vocabulary) for _ in range(len(train_data))]\n",
    "\n",
    "    for x in range(len(train_data)):\n",
    "        for y in range(len(train_data['content'][x].split())):\n",
    "            words_in_one_sentence = (train_data['content'][x].split()[y]).lower().translate(translation_table)\n",
    "            if words_in_one_sentence != '':   \n",
    "                index_of_word = unique_vocabulary.index(words_in_one_sentence)\n",
    "                binary_bag[x][index_of_word] = 1\n",
    "\n",
    "    # sigma(z) = 1 / (1 + e^(-z))\n",
    "\n",
    "    num_docs_including_word = [0]*len(unique_vocabulary)\n",
    "    num_pos_docs_including_word = [0]*len(unique_vocabulary)\n",
    "    num_neg_docs_including_word = [0]*len(unique_vocabulary)\n",
    "\n",
    "    for word in range(len(unique_vocabulary)):\n",
    "        for document in range(len(binary_bag)):\n",
    "            if binary_bag[document][word] == 1:\n",
    "                num_docs_including_word[word] += 1\n",
    "                if document in pos_indices:\n",
    "                    num_pos_docs_including_word[word] += 1\n",
    "                if document in neg_indices:\n",
    "                    num_neg_docs_including_word[word] += 1\n",
    "    \n",
    "    prob_word_given_pos_class = []\n",
    "    prob_word_given_neg_class = []\n",
    "\n",
    "    count_pos_class = math.fsum(num_pos_docs_including_word)\n",
    "    count_neg_class = math.fsum(num_neg_docs_including_word)\n",
    "\n",
    "\n",
    "    for x in range(len(unique_vocabulary)):\n",
    "        prob_word_given_pos_class.append(float(num_pos_docs_including_word[x] + 1) / float(count_pos_class + len(unique_vocabulary)))\n",
    "        prob_word_given_neg_class.append(float(num_neg_docs_including_word[x] + 1) / float(count_neg_class + len(unique_vocabulary)))        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve with both models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# get probabilities for each review\n",
    "pos_probs_no_stop_words = []\n",
    "neg_probs_no_stop_words = []\n",
    "pos_probs = []\n",
    "neg_probs = []\n",
    "\n",
    "for x in range(len(test_data)):\n",
    "    pos_prob, neg_prob = naive_bayes_individual_no_stop_words(test_data['content'][x])\n",
    "    pos_probs_no_stop_words.append(pos_prob)\n",
    "    neg_probs_no_stop_words.append(neg_prob)\n",
    "    \n",
    "    pos_prob, neg_prob = naive_bayes_individual(test_data['content'][x])\n",
    "    pos_probs.append(pos_prob)\n",
    "    neg_probs.append(neg_prob)\n",
    "\n",
    "# get actual labels\n",
    "actual_labels = test_data['review_class'].tolist()\n",
    "\n",
    "# get fpr, tpr, and thresholds for both models\n",
    "fpr_no_stop_words, tpr_no_stop_words, thresholds_no_stop_words = roc_curve(actual_labels, pos_probs_no_stop_words, pos_label='positive')\n",
    "roc_auc_no_stop_words = auc(fpr_no_stop_words, tpr_no_stop_words)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(actual_labels, pos_probs, pos_label='positive')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr_no_stop_words, tpr_no_stop_words, color='darkorange', lw=2, label='ROC curve without stop words (area = %0.2f)' % roc_auc_no_stop_words)\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve with stop words (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
